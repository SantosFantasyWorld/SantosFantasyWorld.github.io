# 线性回归

这里的推导过程是我用最简数据挖掘的教材结合沐神的教程总结而来

### 什么是线性回归

线性回归是一种机器学习算法，用于预测连续变量的值。它假设自变量与因变量之间存在线性关系，通过拟合一个线性模型来预测因变量的值。

例如，我们想预测房屋的售价，可能会考虑以下自变量：房屋的面积、所在城市、房龄等。我们可以采集一些已知售价和自变量的房屋数据，利用线性回归算法来构建一个模型，从而预测新房屋的售价。

数学上，线性回归的模型可以表示为：

$$y = \beta_0 + \beta_1x_1 + \beta_2x_2 + ... + \beta_nx_n + \epsilon$$

其中y表示因变量的值，

$x_1, x_2, \cdots, x_n$分别表示 $p$ 个自变量的值，

$\beta_0, \beta_1, \beta_2, \cdots, \beta_n$是模型的系数而

$$\epsilon$$

是误差项。

我们的目标是找到一组最优的系数，使得模型能够最好地拟合数据。这可以通过最小化误差的平方和来实现，即最小二乘法。

### 线性回归的推导过程

首先，搞清楚一件事，我们到底要推导什么？

线性回归其实推导的就是

$$\beta_0, \beta_1, \beta_2, \cdots, \beta_n$$

我们来推导线性回归的过程。首先，定义损失函数（平方损失）为：

$$L(\beta) = \sum_{i=1}^{m} (y^{(i)} - (\beta_0 + \beta_1x^{(i)}_1 + ... + \beta_nx^{(i)}_n))^2$$

要找到使损失函数最小化的所有参数 $\beta_0, \beta_1, \beta_2, \cdots, \beta_n$，我们可以对损失函数求梯度，并使梯度为零。对于 $\beta_j$，梯度计算如下：

$$\frac{\partial L(\beta)}{\partial \beta_j} = -2 \sum_{i=1}^{m}  (y^{(i)} - (\beta_0 + \beta_1x^{(i)}_1 + ... +  \beta_nx^{(i)}_n))x^{(i)}_j$$

使梯度为零，即求解以下方程组：

$$\frac{\partial L(\beta)}{\partial \beta_0} = 0, \frac{\partial L(\beta)}{\partial \beta_1} = 0, ..., \frac{\partial L(\beta)}{\partial \beta_n} = 0$$

将上述方程组写成矩阵形式：

$$X^TX\beta = X^Ty$$

其中，$X$ 是输入矩阵，$y$ 是因变量向量。最后，我们可以通过求解线性方程组得到最优参数 $\beta$：

$$\beta = (X^TX)^{-1}X^Ty$$

接下来，我们用一个简单的例子详细解释线性回归。假设我们有以下数据点：

| x    | y    |
| ---- | ---- |
| 1    | 2    |
| 2    | 4    |
| 3    | 6    |

我们要拟合一个简单的线性回归模型：

$$y = \beta_0 + \beta_1x$$

根据上面的推导，我们可以计算最优参数 $\beta$。首先，构造输入矩阵 $X$ 并且往列向量里面加入了常数1，这样就相当于$y = \beta_0*1 + \beta_1x$

$$X^TX = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
1 & 3 \\
\end{bmatrix}^T
\begin{bmatrix}
1 & 1 \\
1 & 2 \\
1 & 3 \\
\end{bmatrix} =
\begin{bmatrix}
3 & 6 \\
6 & 14 \\
\end{bmatrix}$$

然后：

$$X^Ty = \begin{bmatrix}
1 & 1 \\
1 & 2 \\
1 & 3 \\
\end{bmatrix}^T
\begin{bmatrix}
2 \\
4 \\
6 \\
\end{bmatrix} =
\begin{bmatrix}
12 \\
26 \\
\end{bmatrix}$$

接下来，我们求解线性方程组 $(X^TX)\beta = X^Ty$：

$$\begin{bmatrix}
3 & 6 \\
6 & 14 \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\end{bmatrix} =
\begin{bmatrix}
12 \\
26 \\
\end{bmatrix}$$

求解该方程组，我们可以得到最优参数 $\beta_0$ 和 $\beta_1$：

$$\beta = (X^TX)^{-1}X^Ty = \begin{bmatrix}
\beta_0 \\
\beta_1 \\
\end{bmatrix} =
\begin{bmatrix}
0 \\
2 \\
\end{bmatrix}$$

**BUT**,你可能会有疑问：:question::question::question:

从这里

$$\frac{\partial L(\beta)}{\partial \beta_0} = 0, \frac{\partial L(\beta)}{\partial \beta_1} = 0, ..., \frac{\partial L(\beta)}{\partial \beta_n} = 0$$

到这里

$$X^TX\beta = X^Ty$$

是怎么来的？

这个推导过程太多了，可以查看西瓜书或者最简数据挖掘的教材:cry::cry::cry:

### 梯度下降

首先我们要清楚为什么要有梯度下降这个玩意，仔细观察以下我在上面给出的例子，x和y才只有三个值，计算都这么麻烦，如果有一万个，甚至十万个怎么办？，计算机虽然很强，但是要计算这么庞大的数据，他也很难的，而且，如果你观察的足够仔细的话，你会发现

$$\begin{bmatrix}
3 & 6 \\
6 & 14 \\
\end{bmatrix}
\begin{bmatrix}
\beta_0 \\
\beta_1 \\
\end{bmatrix} =
\begin{bmatrix}
12 \\
26 \\
\end{bmatrix}$$

如果这个例子的$\beta$ 的左边的数据不是一个2行2列的矩阵怎么办，就算是个2行2列的矩阵，如果第一行或者第二行的数据全是0怎么办？算不出来啊:cry::cry::cry:

$$X^TX\beta = X^Ty$$

专业的说法是上式存在唯一最优解的前提是$X^TX$存在逆。也不需要太理解这段话，意思就是，硬着算很困难，也不一定能算出来

那么，另一个思路来了:**梯度下降**

梯度下降是一种优化算法，用于求解机器学习和深度学习中的损失函数最小值。它通过不断地调整模型参数（如权重和偏置），沿着梯度的负方向更新参数，以最小化损失函数。

梯度下降的基本思想是计算损失函数关于模型参数的梯度，然后按照这个梯度的负方向更新参数。这个过程可以用以下公式表示：

$$\theta_{t+1} = \theta_t - \alpha \nabla_\theta J(\theta_t)$$

其中$\theta$ 表示模型参数而$t$ 为迭代次数，另一个参数$\alpha$ 是学习率（一个正数，而$J(\theta)$ 是损失函数，最后$\nabla_\theta J(\theta)$ 是损失函数关于参数的梯度。

栗子：

假设我们有一组数据点 $(x_i, y_i)$，我们想用一条直线 $y = wx + b$ 来拟合这些数据点。我们可以定义一个损失函数来衡量拟合的误差，这里我们使用均方误差作为损失函数：

$$J(w, b) = \frac{1}{N} \sum_{i=1}^{N} (y_i - (wx_i + b))^2$$

我们的目标是找到最优的 $w$ 和 $b$，使得损失函数 $J(w, b)$ 最小化。我们可以使用梯度下降算法来实现这个目标。

首先，我们需要计算损失函数关于参数 $w$ 和 $b$ 的梯度：

$$\frac{\partial J}{\partial w} = -\frac{2}{N} \sum_{i=1}^{N} x_i(y_i - (wx_i + b))$$

$$\frac{\partial J}{\partial b} = -\frac{2}{N} \sum_{i=1}^{N} (y_i - (wx_i + b))$$

然后，我们使用梯度下降算法来更新参数 $w$ 和 $b$：

$$w_{t+1} = w_t - \alpha \frac{\partial J}{\partial w}(w_t, b_t)$$

$$b_{t+1} = b_t - \alpha \frac{\partial J}{\partial b}(w_t, b_t)$$

重复上述迭代过程，直到损失函数的值不再明显下降或达到预设的迭代次数，我们就可以找到使损失函数最小化的参数 $w$ 和 $b$。



# Softmax 函数

### 基本概念

Softmax 函数是一个常用于机器学习和深度学习的激活函数，特别是在多分类问题中。Softmax 函数的主要作用是将一组实数转换成概率分布，这样就可以找到输入数据属于不同类别的概率。Softmax 函数的输出概率之和为1，使得输出更具有可解释性。此外，Softmax 函数是可微的，这对于梯度下降等优化算法来说非常重要。

Softmax 函数的定义如下：

给定一个向量 $z = (z_1, z_2, \dots, z_n)$，Softmax 函数的输出为：
$$
\sigma(z)_i = \frac{e^{z_i}}{\sum_{j=1}^n e^{z_j}}
$$
其中，$i \in {1, 2, \dots, n}$。

现在我们用一个例子详细说明：

假设一个多分类问题有3个类别，某个样本的模型输出（logits）为 $z = (1.0, 2.0, 3.0)$。我们首先计算所有输出指数的和：

$$j= \sum_{j=1}^3 e^{z_j} = e^{1.0} + e^{2.0} + e^{3.0} \approx 30.192$$

然后计算 Softmax 函数的输出：

$$\sigma(z)_1 = \frac{e^{1.0}}{30.192} \approx 0.090$$

$$\sigma(z)_2 = \frac{e^{2.0}}{30.192} \approx 0.245$$

$$\sigma(z)_3 = \frac{e^{3.0}}{30.192} \approx 0.665$$

所以，通过 Softmax 函数转换后，我们得到的概率分布为：$(0.090, 0.245, 0.665)$。这意味着，这个样本属于第1类的概率约为 9.0%，属于第2类的概率约为 24.5%，属于第3类的概率约为 66.5%。从这个概率分布中，我们可以判断该样本最有可能属于第3类

### 交叉熵损失

Softmax 函数和交叉熵损失在多分类问题中通常一起使用。Softmax 函数将模型的原始输出（logits）转换成概率分布，而交叉熵损失则用于衡量这个概率分布与真实类别的标签分布之间的差异。当模型的预测概率分布与实际标签分布越接近时，交叉熵损失值越小。因此，通过优化交叉熵损失，我们可以训练出能够准确预测类别的模型。

假设有 $n$ 个类别，模型的预测概率分布为 $\hat{y} = (\hat{y}_1, \hat{y}_2, \dots, \hat{y}_n)$，其中 $\hat{y}_i$ 是模型预测的第 $i$ 个类别的概率。实际的标签分布为 $y = (y_1, y_2, \dots, y_n)$，其中 $y_i$ 为第 $i$ 个类别的真实概率。在多分类问题中，通常使用 one-hot 编码表示真实标签，即真实类别对应的元素为 1，其他元素为 0。

交叉熵损失是一种常用的损失函数，主要应用于分类问题，特别是概率分布之间的比较。它衡量两个概率分布之间的差异，通常用于评估分类器预测的概率分布与实际标签分布之间的接近程度。交叉熵损失的定义如下：

$$-Σy_i * log(p_i)$$

其中 $y_i$ 是实际标签分布（一般为 one-hot 编码，即正确类别为1，其他类别为0），$p_i$ 是预测概率分布。交叉熵损失越小，表示预测概率分布越接近实际标签分布。

现在我们用一个实际例子来详细解释交叉熵损失：

假设我们有一个3分类问题，比如对动物进行分类：猫、狗和鸟。给定一个样本，我们的分类器输出以下概率分布：

预测概率分布：猫 - 0.7，狗 - 0.2，鸟 - 0.1

实际标签分布：假设这个样本是一只猫，那么实际标签分布为：猫 - 1，狗 - 0，鸟 - 0（one-hot 编码）

接下来，我们计算交叉熵损失：

交叉熵损失 = $- Σ y_i * log(p_i] = - (1 * log(0.7) + 0 * log(0.2) + 0 * log(0.1)) ≈ 0.36$

在训练过程中，我们的目标是最小化交叉熵损失。通过调整模型参数（如权重和偏置项），我们希望使预测概率分布更接近实际标签分布，从而降低交叉熵损失。当交叉熵损失趋近于最小值时，模型的预测性能通常会达到最佳。

### 损失函数

注意，我们前面讲的Softmax 函数和交叉熵损失都是用来计算预测概率分布的，为了防止过拟合，我们还必须搭配损失函数来进行使用

L1损失、L2损失和Huber损失是三种常用的损失函数，它们主要用于回归问题。下面我们将分别对这三种损失函数进行解释，并提供图表和例子。

##### L1损失（绝对误差损失）：


L1损失是实际值与预测值之间的绝对差值。公式如下：

L1损失 = |y - ŷ|

L1损失对异常值不敏感，因此在存在异常值的情况下表现较好。但是，在接近最优解时，L1损失的收敛速度较慢。L1损失的图像类似于V型，其斜率随着误差的减小而减小。

##### L2损失（均方误差损失）：
L2损失是实际值与预测值之间差值的平方。公式如下：

L2损失 = (y - ŷ)^2

L2损失对异常值敏感，因此在没有异常值的情况下表现较好。与L1损失相比，L2损失在接近最优解时收敛速度较快。L2损失的图像类似于U型，其斜率随着误差的减小而减小。

##### Huber损失（Huber's Robust Loss）：
Huber损失是L1损失和L2损失的组合。对于较小的误差，Huber损失表现为L2损失；对于较大的误差，表现为L1损失。Huber损失的公式如下：

Huber损失(delta) = 0.5 * (y - ŷ)^2, 当 |y - ŷ| <= delta
Huber损失(delta) = delta * |y - ŷ| - 0.5 * delta^2, 当 |y - ŷ| > delta

delta是一个超参数，用于控制误差从L2损失过渡到L1损失的界限。Huber损失在处理存在异常值的数据时表现较好，同时在接近最优解时收敛速度较快。Huber损失的图像在误差较小时类似于U型，误差较大时类似于V型。

例子：
假设我们有一个简单的回归问题，实际值y=4，预测值ŷ=3。现在我们计算三种损失函数的值。

L1损失 = |4 - 3| = 1
L2损失 = (4 - 3)^2 = 1
Huber损失(delta=1) = 0.5 * (4 - 3)^2 = 0.5

在这个例子中，我们可以看到L1损失的值在这个例子中，我们可以看到L1损失的值为1，L2损失的值为1，而Huber损失（设定delta=1）的值为0.5。下面是对这三种损失函数在不同误差范围内的表现的简要分析：

当误差较小（例如 |y - ŷ| <= 1）时：
    L1损失会导致较慢的收敛速度。
    L2损失的收敛速度较快，但在存在异常值时可能受到较大影响。
    Huber损失在这种情况下的表现类似于L2损失，收敛速度较快。

当误差较大（例如 |y - ŷ| > 1）时：
    L1损失对异常值不敏感，因此在这种情况下表现较好。
    L2损失受异常值影响较大，可能导致模型过度拟合异常值。
    Huber损失在这种情况下的表现类似于L1损失，对异常值不敏感。

因此，在选择损失函数时，我们需要根据数据的特点和具体问题来权衡。例如，对于没有异常值的数据，可以选择L2损失；对于存在异常值的数据，可以选择L1损失或Huber损失。Huber损失在很多情况下是一个很好的折衷方案，既能应对异常值，又能保证收敛速度。



