# 为什么要用tensor, 而不是numpy 库

Tensor是基于NumPy的一个扩展库，它支持多维数组和矩阵运算，可以使用GPU进行加速计算。Tensor最初是为神经网络和深度学习而设计的，但现在已经成为许多其他科学计算领域的常用工具。Tensor与NumPy类似，但支持更多的操作和数据类型，包括张量、自动微分、卷积神经网络等。

下面是一个使用Tensor和NumPy进行矩阵乘法的示例代码，同时记录运行时间，以证明Tensor相对于NumPy在GPU上的加速效果：

~~~
import torch
import numpy as np
import time

# 定义两个矩阵
A = np.random.rand(10000, 10000).astype(np.float32)
B = np.random.rand(10000, 10000).astype(np.float32)

# 使用NumPy进行矩阵乘法
start = time.time()
C_np = np.dot(A, B)
end = time.time()
print('NumPy matrix multiplication took:', end - start, 'seconds')
~~~

输出的结果为：

~~~
NumPy matrix multiplication took: 33.43119215965271 seconds
~~~

但是当我们把这个矩阵转换为Tensor，然后再进行计算：

~~~
# 将NumPy数组转换为Tensor
A_t = torch.from_numpy(A)
B_t = torch.from_numpy(B)

# 将Tensor放入GPU中
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
A_t = A_t.to(device)
B_t = B_t.to(device)

# 使用Tensor进行矩阵乘法
start = time.time()
C_t = torch.matmul(A_t, B_t)
C_t = C_t.to('cpu')
C_t = C_t.numpy()
end = time.time()
print('Tensor matrix multiplication took:', end - start, 'seconds')

# 验证结果是否相同
print(np.allclose(C_np, C_t))
~~~

输出结果为：

~~~
Tensor matrix multiplication took: 3.7803730964660645 seconds
True
~~~

快了得有7，8倍，！！！:accept::accept::accept:

**另外**，numpy 和 tensor 其中的一个显著差异是两者**广播机制的不同**，李沐的书里有特意讲到这一点，以下是自己的理解：

~~~
import numpy as np
import torch

# 使用NumPy进行广播
a_np = np.array([[1], [2], [3]])
b_np = np.array([4, 5, 6])
c_np = a_np + b_np
print(c_np)
~~~

结果为：

~~~
[[5 6 7]
 [6 7 8]
 [7 8 9]]
~~~

但是如果使用tensor:

~~~
# 使用Tensor进行广播
a_t = torch.tensor([[1], [2], [3]])
b_t = torch.tensor([4, 5, 6])
c_t = a_t + b_t
print(c_t)
~~~

结果为：

~~~
tensor([[5, 6, 7],
        [6, 7, 8],
        [7, 8, 9]])
~~~

与NumPy相比，Tensor的广播机制在某些情况下会表现得更加智能和灵活。例如，Tensor可以广播多个维度，而NumPy只能广播一个维度。



# 线性代数

1. 标量

标量是只有一个数值的量，通常用小写字母表示。在数学中，标量是一种没有方向和大小的量，只有一个数值的属性。

- 一个长度为3的向量的标量表示其大小。

  - 温度、重量、密度等都是标量。

2. 向量

向量是有方向和大小的量，通常用粗体的小写字母表示。向量是由一组有序的数列组成，通常表示为一列或一行，也就是一个一维数组。在数学中，向量通常表示空间中的一个点，该点的坐标是由向量的各个元素表示的。

- 向量可以用于表示速度、力、加速度等物理量。

- 三维空间中的位置向量、速度向量、力向量等都是向量。

3. 矩阵

矩阵是一个二维数组，由多个数值按照一定的排列方式排列而成，通常用大写字母表示。矩阵的大小由其行数和列数确定。矩阵可以用于表示线性变换、映射和方程组。

- 2x2矩阵、3x3矩阵等。

4. 张量

张量是一个多维数组，可以看作是向量和矩阵的扩展，通常用大写字母表示。在机器学习和深度学习中，张量是一种多维数组，用于表示神经网络中的输入、输出和参数。张量在深度学习中具有重要的作用，因为神经网络中的大部分计算都可以看作是张量之间的乘积、加法、卷积等运算。

- 一张RGB图像可以表示为一个3维张量，其中第一维表示图像的行，第二维表示图像的列，第三维表示像素的RGB通道。在PyTorch中，一个张量可以有任意维度，可以是一个标量、向量、矩阵或更高维的张量。

5. 点积

点积是向量之间的一种运算，也称为数量积。两个向量的点积定义为它们各个元素的乘积之和。通常用“.”表示。如果两个向量的点积为0，则称它们正交。

- 假设有向量A=[1,2,3]和B=[4,5,6]，则它们的点积为1x4+2x5+3x6=32。

6. 矩阵乘法

矩阵乘法是将两个矩阵相乘以得到一个新的矩阵。要使两个矩阵相乘，第一个矩阵的列数必须与第二个矩阵的行数相等。矩阵乘法满足结合律、分配律和与标量乘法的结合律，但不满足交换律。

- 设有矩阵 A = [ [1, 2], [3, 4] ] 和 B = [ [5, 6], [7, 8] ]，矩阵乘法计算如下： AB = [ [(1 * 5) + (2 * 7), (1 * 6) + (2 * 8)], [(3 * 5) + (4 * 7), (3 * 6) + (4 * 8)] ] = [ [19, 22], [43, 50] ]

7. 范数

范数是一个函数，用于衡量向量的大小。常见的范数有 L1 范数（曼哈顿距离）、L2 范数（欧几里得距离）等。L1 范数是向量元素的绝对值之和，而 L2 范数是向量元素的平方和的平方根。范数具有非负性、齐次性和三角不等式等性质。

- 设有向量 A = [3, -4]，L1 范数计算如下： ‖A‖₁ = |3| + |-4| = 3 + 4 = 7；L2 范数计算如下： ‖A‖₂ = √(3² + (-4)²) = √(9 + 16) = √25 = 5

好多内容:tired_face::tired_face::tired_face:



# 矩阵导数运算

### 标量与标量求导

标量导数（Scalar Derivative）是一个函数关于其自变量的变化率。换句话说，导数表示函数在某一点处的斜率。对于单变量函数，我们通常用 $$f'(x)$$ 或 $$df(x)/dx$$ 来表示其导数。

让我们通过一个例子详细解释标量导数：

考虑一个函数$$f(x) = x^3 + 3x^2 - 2x + 1$$。我们希望求解 f(x) 关于 x 的导数。

其标量导数为：$$ f'(x) = d(x^3)/dx + d(3x^2)/dx - d(2x)/dx + d(1)/dx$$

也就是：$$ f'(x) = 3x^2 + 6x - 2$$

**这个其实就等于普通求导**

### 标量与向量求导（标量是分子，向量是分母）

标量与向量求导其实就是指梯度。梯度是一个向量，它表示一个标量函数关于向量变量的导数。梯度表示函数在某一点处的最大增长方向，梯度的大小表示函数增长的速度。

假设有一个标量函数 f(x, y)，我们需要求关于向量变量 (x, y) 的导数。梯度表示为∇f 或 grad f。梯度是一个向量，其分量是标量函数关于向量变量的偏导数。对于这个例子，梯度是：

$$∇f(x,y)=(∂f/∂x,∂f/∂y)$$

例子：考虑一个标量函数 $f(x, y) = x^2 + xy + y^2$，我们希望求解 f 关于向量 (x, y) 的梯度。

首先，我们需要计算偏导数。偏导数表示函数关于某个变量的导数，同时将其他变量视为常数。

$$∂f/∂x=d(x^2+xy*+y^2)/dx=2x+y$$

$$∂f/∂y=d(x^2+xy+y^2)/dy=x+2y$$

现在，我们可以求梯度：

$$∇f(x,y)=(∂f/∂x,∂f/∂y)=(2x+y,x+2y)$$

所以，函数 $f(x, y) = x^2 + xy + y^2$ 的梯度是 $\nabla f(x, y) = (2x + y, x + 2y)$。这个梯度向量表示了函数在任意点 (x, y) 处的最大增长方向，其大小表示函数增长的速度。

### 向量与标量求导（标量是分母，向量是分子）

向量与标量求导通常是指方向导数。这个其实没有什么好讲的，就是拿向量中的每一个变量与标量求导。得出来的向量的shape还是与原向量相同

### 向量与向量求导

向量与向量求导是指雅可比矩阵。雅可比矩阵表示一个向量值函数关于其向量变量的导数。设有一个向量值函数 $\mathbf{F}(\mathbf{x}) = (f_1(\mathbf{x}), f_2(\mathbf{x}), \dots, f_n(\mathbf{x}))$，其中 $\mathbf{x} = (x_1, x_2, \dots, x_m)$ 是一个向量。雅可比矩阵由偏导数组成，形状为 $n \times m$，定义如下：

$$
\mathbf{J}(\mathbf{x}) = \begin{bmatrix}
\frac{\partial f_1}{\partial x_1} & \frac{\partial f_1}{\partial x_2} & \dots & \frac{\partial f_1}{\partial x_m} \\
\frac{\partial f_2}{\partial x_1} & \frac{\partial f_2}{\partial x_2} & \dots & \frac{\partial f_2}{\partial x_m} \\
\vdots & \vdots & \ddots & \vdots \\
\frac{\partial f_n}{\partial x_1} & \frac{\partial f_n}{\partial x_2} & \dots & \frac{\partial f_n}{\partial x_m}
\end{bmatrix}
$$

例子：考虑一个向量值函数 $\mathbf{F}(x, y) = \begin{bmatrix} x^2 + y^2 \\ x + y \end{bmatrix}$，我们希望求解 $\mathbf{F}$ 关于向量 $(x, y)$ 的雅可比矩阵。

首先，我们需要计算每个分量关于 $(x, y)$ 的偏导数：

$$
\frac{\partial}{\partial x} \begin{bmatrix} x^2 + y^2 \\ x + y \end{bmatrix} = \begin{bmatrix} 2x \\ 1 \end{bmatrix}, \quad \frac{\partial}{\partial y} \begin{bmatrix} x^2 + y^2 \\ x + y \end{bmatrix} = \begin{bmatrix} 2y \\ 1 \end{bmatrix}
$$

现在我们可以构建雅可比矩阵： $$ \mathbf{J}(x, y) = \begin{bmatrix} \frac{\partial}{\partial x}(x^2 + y^2) & \frac{\partial}{\partial y}(x^2 + y^2) \\ \frac{\partial}{\partial x}(x + y) & \frac{\partial}{\partial y}(x + y) \end{bmatrix} = \begin{bmatrix} 2x & 2y \\ 1 & 1 \end{bmatrix} $$ 所以，向量值函数 $\mathbf{F}(x, y) = \begin{bmatrix} x^2 + y^2 \\ x + y \end{bmatrix}$ 关于向量 $(x, y)$ 的雅可比矩阵是： $$ \mathbf{J}(x, y) = \begin{bmatrix} 2x & 2y \\ 1 & 1 \end{bmatrix} $$ 雅可比矩阵描述了向量值函数在特定点的局部变化率。

沐神这里还是有很多没有讲清楚得地方，这些是我自己从网上找资料总结来的:no_good::no_good::no_good:



# 自动求导

### 向量链式法则

向量链式法则是用于求解复合向量值函数导数的方法。假设我们有两个向量值函数 $\mathbf{F}(\mathbf{x})$ 和 $\mathbf{G}(\mathbf{y})$，其中 $\mathbf{x} = (x_1, x_2, \dots, x_m)^T$ 和 $\mathbf{y} = (y_1, y_2, \dots, y_n)^T$ 是向量。我们有一个复合函数 $\mathbf{H}(\mathbf{x}) = \mathbf{F}(\mathbf{G}(\mathbf{x}))$，向量链式法则可以用来计算 $\mathbf{H}$ 关于 $\mathbf{x}$ 的导数。

向量链式法则的形式如下：

$$
\frac{d\mathbf{H}}{d\mathbf{x}} = \frac{d\mathbf{F}}{d\mathbf{y}} \frac{d\mathbf{G}}{d\mathbf{x}}
$$

其中，$\frac{d\mathbf{F}}{d\mathbf{y}}$ 是 $\mathbf{F}$ 关于 $\mathbf{y}$ 的雅可比矩阵，$\frac{d\mathbf{G}}{d\mathbf{x}}$ 是 $\mathbf{G}$ 关于 $\mathbf{x}$ 的雅可比矩阵。

例子：假设我们有两个向量值函数 $\mathbf{F}(u, v) = \begin{bmatrix} u^2 + v \\ uv \end{bmatrix}$ 和 $\mathbf{G}(x, y) = \begin{bmatrix} x + y \\ x^2 - y^2 \end{bmatrix}$。现在我们想求解复合函数 $\mathbf{H}(x, y) = \mathbf{F}(\mathbf{G}(x, y))$ 关于向量 $(x, y)$ 的导数。

首先，我们需要求解 $\mathbf{F}$ 和 $\mathbf{G}$ 的雅可比矩阵：

$$
\mathbf{J}_F(u, v) = \begin{bmatrix} \frac{\partial}{\partial u}(u^2 + v) & \frac{\partial}{\partial v}(u^2 + v) \\ \frac{\partial}{\partial u}(uv) & \frac{\partial}{\partial v}(uv) \end{bmatrix} = \begin{bmatrix} 2u & 1 \\ v & u \end{bmatrix}
$$

$$
\mathbf{J}_G(x, y) = \begin{bmatrix} \frac{\partial}{\partial x}(x + y) & \frac{\partial}{\partial y}(x + y) \\ \frac{\partial}{\partial x}(x^2 - y^2) & \frac{\partial}{\partial y}(x^2 - y^2) \end{bmatrix} = \begin{bmatrix} 1 & 1 \\ 2x & -2y \end{bmatrix}
$$

现在，我们用 $\mathbf{G}(x,y)$ 代替 $(u, v)$，并将结果代入 $\mathbf{F}$ 中： $$ \mathbf{H}(x, y) = \mathbf{F}(\mathbf{G}(x, y)) = \mathbf{F}(x + y, x^2 - y^2) = \begin{bmatrix} (x + y)^2 + (x^2 - y^2) \\ (x + y)(x^2 - y^2) \end{bmatrix} $$ 接下来，我们应用向量链式法则计算 $\frac{d\mathbf{H}}{d\mathbf{x}}$： $$ \frac{d\mathbf{H}}{d\mathbf{x}} = \mathbf{J}_F(\mathbf{G}(x, y)) \mathbf{J}_G(x, y) = \mathbf{J}_F(x + y, x^2 - y^2) \mathbf{J}_G(x, y) $$ 注意，这里我们需要计算矩阵的乘积，因此 $\mathbf{J}_F(\mathbf{G}(x, y))$ 的列数必须等于 $\mathbf{J}_G(x, y)$ 的行数。在这个例子中，这个条件得到满足，因为 $\mathbf{J}_F$ 是一个 $2 \times 2$ 矩阵，$\mathbf{J}_G$ 也是一个 $2 \times 2$ 矩阵。 接下来，我们计算乘积： $$ \frac{d\mathbf{H}}{d\mathbf{x}} = \begin{bmatrix} 2(x + y) & 1 \\ x^2 - y^2 & x + y \end{bmatrix} \begin{bmatrix} 1 & 1 \\ 2x & -2y \end{bmatrix} = \begin{bmatrix} 2(x + y) + 2x & 2(x + y) - 2y \\ (x^2 - y^2) + 2x(x + y) & (x^2 - y^2) - 2y(x+y) \end{bmatrix} $$ 简化后： $$ \frac{d\mathbf{H}}{d\mathbf{x}} = \begin{bmatrix} 4x + 2y & 2x \\ 3x^2 + 2xy - y^2 & x^2-3y^2-2xy \end{bmatrix} $$ 这就是复合函数 $\mathbf{H}(x, y)$ 关于向量 $(x, y)$ 的导数。

### 正向累积和反向累积

其实两者用的都是链式法则得原理只是方向不一样。

在正向累积中，我们从输入变量开始，逐步计算函数各个部分的导数。正向累积是一种局部到全局的方法，逐步将局部导数合并到全局导数中。这种方法在计算单个输出关于多个输入的导数时是非常高效的。正向累积的主要缺点是，当我们需要计算多个输出关于单个输入的导数时，计算成本会随着输出数量的增加而线性增长。

在反向累积中，我们首先计算函数值，然后从输出变量开始，逐步计算函数各个部分的导数。反向累积是一种全局到局部的方法，逐步将全局导数分解为局部导数。这种方法在计算多个输出关于单个输入的导数时是非常高效的，因此在深度学习和神经网络中广泛使用，特别是在计算梯度下降中的梯度时。反向累积的主要缺点是，当我们需要计算单个输出关于多个输入的导数时，计算成本会随着输入数量的增加而线性增长。

完事大吉！！！！:open_mouth::open_mouth::open_mouth:
